{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "data_training = pandas.read_csv('Train1.csv') \n",
    "data_testing = pandas.read_csv('valid1.csv') \n",
    "data_final = pandas.read_csv('final1.csv') \n",
    "group = data_training[['mvar33','actual_vote']].groupby('mvar33').agg(lambda x: ','.join(x)).apply(list)\n",
    "from pandas import DataFrame\n",
    "from collections import Counter\n",
    "dicl = DataFrame.to_dict(group)\n",
    "zips = {}\n",
    "majority = {}\n",
    "for i in dicl['actual_vote'].keys():\n",
    "    l = dicl['actual_vote'][i].split(',')\n",
    "    majority[i] = [l.count('Ebony'),l.count('Odyssey'),l.count('Tokugawa'),l.count('Cosmos'),l.count('Centaur')]\n",
    "    if len(Counter(l)) == 1:\n",
    "        zips[i] = [Counter(l).most_common(1)[0][0],Counter(l).most_common(1)[0][1],'Noone',0]\n",
    "    else:\n",
    "        zips[i] = [Counter(l).most_common(1)[0][0],Counter(l).most_common(1)[0][1],Counter(l).most_common(2)[1][0],Counter(l).most_common(2)[1][1]]        \n",
    "df1 = pandas.read_csv('Train1.csv') \n",
    "df2 = pandas.read_csv('valid1.csv')\n",
    "df3 = pandas.read_csv('final1.csv')\n",
    "frames = [df1, df2, df3]\n",
    "df4 = pandas.concat(frames)\n",
    "x = set(df4['mvar33'].values)\n",
    "y = x - set(df1['mvar33'].values) \n",
    "group = df4[['mvar33','party_voted_past']].groupby('mvar33').agg(lambda x: ','.join(x)).apply(list)\n",
    "from pandas import DataFrame\n",
    "from collections import Counter\n",
    "dicl = DataFrame.to_dict(group)\n",
    "for i in list(y):\n",
    "    l = dicl['party_voted_past'][i].split(',')\n",
    "    majority[i] = [l.count('Ebony'),l.count('Odyssey'),l.count('Tokugawa'),l.count('Cosmos'),l.count('Centaur')]\n",
    "    if len(Counter(l)) == 1:\n",
    "        zips[i] = [Counter(l).most_common(1)[0][0],Counter(l).most_common(1)[0][1],'Noone',0]\n",
    "    else:\n",
    "        zips[i] = [Counter(l).most_common(1)[0][0],Counter(l).most_common(1)[0][1],Counter(l).most_common(2)[1][0],Counter(l).most_common(2)[1][1]]        \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "x = set(df4['mvar32'].values)\n",
    "y = x - set(df1['mvar33'].values) \n",
    "group = df4[['mvar32','party_voted_past']].groupby('mvar32').agg(lambda x: ','.join(x)).apply(list)\n",
    "from pandas import DataFrame\n",
    "from collections import Counter\n",
    "dicl = DataFrame.to_dict(group)\n",
    "for i in list(y):\n",
    "    l = dicl['party_voted_past'][i].split(',')\n",
    "    majority[i] = [l.count('Ebony'),l.count('Odyssey'),l.count('Tokugawa'),l.count('Cosmos'),l.count('Centaur')]\n",
    "    if len(Counter(l)) == 1:\n",
    "        zips[i] = [Counter(l).most_common(1)[0][0],Counter(l).most_common(1)[0][1],'Noone',0]\n",
    "    else:\n",
    "        zips[i] = [Counter(l).most_common(1)[0][0],Counter(l).most_common(1)[0][1],Counter(l).most_common(2)[1][0],Counter(l).most_common(2)[1][1]]        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "A = 0.30706475\n",
    "B = 0.2\n",
    "C = 0.1\n",
    "D = 0.2\n",
    "E = 0.17\n",
    "\n",
    "for data_training in [df1,df2,df3]:\n",
    "        data_training['Ebony'] = A*data_training['mvar2'] + B*data_training['mvar7'] + C*data_training['mvar12']  + D*data_training['mvar17'] + E*data_training['mvar22']\n",
    "        data_training['Odyssey'] = A*data_training['mvar4'] + B*data_training['mvar9'] + C*data_training['mvar14']  + D*data_training['mvar19'] +E*data_training['mvar24']\n",
    "        data_training['Tokugawa'] = A*data_training['mvar3'] + B*data_training['mvar8'] + C*data_training['mvar13']  + D*data_training['mvar18'] +E*data_training['mvar23']\n",
    "        data_training['Cosmos'] = A*data_training['mvar5'] + B*data_training['mvar10'] + C*data_training['mvar15']  + D*data_training['mvar20'] +E*data_training['mvar25']\n",
    "        data_training['Centaur'] = A*data_training['mvar1'] + B*data_training['mvar6'] + C*data_training['mvar11']  + D*data_training['mvar16'] +E*data_training['mvar21']\n",
    "        data_training['Ebony1'] = A*data_training['mvar2'] \n",
    "        data_training['Odyssey1'] = A*data_training['mvar4'] \n",
    "        data_training['Tokugawa1'] = A*data_training['mvar3']\n",
    "        data_training['Cosmos1'] = A*data_training['mvar5'] \n",
    "        data_training['Centaur1'] = A*data_training['mvar1']\n",
    "        \n",
    "        data_training['Ebony2'] =  B*data_training['mvar7'] \n",
    "        data_training['Odyssey2'] = B*data_training['mvar9']\n",
    "        data_training['Tokugawa2'] = B*data_training['mvar8']\n",
    "        data_training['Cosmos2'] =  B*data_training['mvar10']\n",
    "        data_training['Centaur2'] = B*data_training['mvar6']\n",
    "        \n",
    "        data_training['Ebony3'] =  C*data_training['mvar12'] \n",
    "        data_training['Odyssey3'] =  C*data_training['mvar14'] \n",
    "        data_training['Tokugawa3'] = C*data_training['mvar13']\n",
    "        data_training['Cosmos3'] =  C*data_training['mvar15'] \n",
    "        data_training['Centaur3'] = C*data_training['mvar11']\n",
    "        \n",
    "        data_training['Ebony4'] =  D*data_training['mvar17'] \n",
    "        data_training['Odyssey4'] =  D*data_training['mvar19']\n",
    "        data_training['Tokugawa4'] = D*data_training['mvar18']\n",
    "        data_training['Cosmos4'] =  D*data_training['mvar20']\n",
    "        data_training['Centaur4'] = D*data_training['mvar16'] \n",
    "        \n",
    "        data_training['Ebony5'] =  E*data_training['mvar22']\n",
    "        data_training['Odyssey5'] =  D*data_training['mvar24']\n",
    "        data_training['Tokugawa5'] = D*data_training['mvar23']\n",
    "        data_training['Cosmos5'] =  D*data_training['mvar25']\n",
    "        data_training['Centaur5'] = D*data_training['mvar21']        \n",
    "        \n",
    "        \n",
    "        Cit_pref1 = []\n",
    "        lev_pref1 = []\n",
    "        lev_pref2 = []\n",
    "        Cit_pref2 = []\n",
    "        city_change = []\n",
    "        ACTIVITY = []\n",
    "        prefs = ['Centaur','Ebony','Tokugawa','Odyssey','Cosmos']\n",
    "        for i in range(len(data_training)):\n",
    "            level4 = [data_training['Centaur4'].values[i],data_training['Ebony4'].values[i],data_training['Tokugawa4'].values[i],data_training['Odyssey4'].values[i],data_training['Cosmos4'].values[i]]\n",
    "            level = [data_training['Centaur'].values[i],data_training['Ebony'].values[i],data_training['Tokugawa'].values[i],data_training['Odyssey'].values[i],data_training['Cosmos'].values[i]]\n",
    "            level1 = [data_training['Centaur1'].values[i],data_training['Ebony1'].values[i],data_training['Tokugawa1'].values[i],data_training['Odyssey1'].values[i],data_training['Cosmos1'].values[i]]\n",
    "            level2 = [data_training['Centaur2'].values[i],data_training['Ebony2'].values[i],data_training['Tokugawa2'].values[i],data_training['Odyssey2'].values[i],data_training['Cosmos2'].values[i]]\n",
    "            level3 = [data_training['Centaur3'].values[i],data_training['Ebony3'].values[i],data_training['Tokugawa3'].values[i],data_training['Odyssey3'].values[i],data_training['Cosmos3'].values[i]]\n",
    "            level4 = [data_training['Centaur4'].values[i],data_training['Ebony4'].values[i],data_training['Tokugawa4'].values[i],data_training['Odyssey4'].values[i],data_training['Cosmos4'].values[i]]\n",
    "            lev_pref1.append(sorted(level)[-1])\n",
    "            Cit_pref1.append(prefs[level.index(sorted(level)[-1])])\n",
    "            lev_pref2.append(sorted(level)[-2])\n",
    "            if sorted(level)[-2] == 0.0:\n",
    "                Cit_pref2.append('NONE')\n",
    "            else:\n",
    "                Cit_pref2.append(prefs[level.index(sorted(level)[-2])])\n",
    "            if data_training['mvar32'].values[i] == data_training['mvar33'].values[i]:\n",
    "                city_change.append(0)\n",
    "            else:\n",
    "                city_change.append(1)    \n",
    "        data_training['Cit_pref1'] = Cit_pref1\n",
    "        data_training['lev_pref1'] = lev_pref1\n",
    "        data_training['lev_pref2'] = lev_pref2\n",
    "        data_training['Cit_pref2'] = Cit_pref2\n",
    "        data_training['city_change'] = city_change\n",
    "        \n",
    "        ZIP_Ebony  = []\n",
    "        ZIP_Odyssey= []\n",
    "        ZIP_Tokugawa= []\n",
    "        ZIP_Cosmos= []\n",
    "        ZIP_Centaur= []\n",
    "        ZIP_pref1 = []\n",
    "        ZIP_pref2 = []\n",
    "        ZIP_lev_pref1 = []\n",
    "        ZIP_lev_pref2 = []\n",
    "        for i in data_training['mvar33'].values:  #    majority[i] = [l.count('Ebony'),l.count('Odyssey'),l.count('Tokugawa'),l.count('Cosmos'),l.count('Centaur')]\n",
    "            ZIP_pref1.append(zips[i][0])\n",
    "            ZIP_pref2.append(zips[i][2])\n",
    "            ZIP_lev_pref1.append(zips[i][1])\n",
    "            ZIP_lev_pref2.append(zips[i][3])                  \n",
    "            ZIP_Ebony.append(majority[i][0])\n",
    "            ZIP_Odyssey.append(majority[i][1])\n",
    "            ZIP_Tokugawa.append(majority[i][2])\n",
    "            ZIP_Cosmos.append(majority[i][3])\n",
    "            ZIP_Centaur.append(majority[i][4])  \n",
    "        data_training['ZIP_pref1'] =    ZIP_pref1\n",
    "        data_training['ZIP_pref2'] =    ZIP_pref2\n",
    "        data_training['ZIP_lev_pref1'] =    ZIP_lev_pref1\n",
    "        data_training['ZIP_lev_pref2'] =    ZIP_lev_pref2\n",
    "        data_training['ZIP_Ebony'] =    ZIP_Ebony\n",
    "        data_training['ZIP_Odyssey'] =    ZIP_Odyssey\n",
    "        data_training['ZIP_Tokugawa'] =    ZIP_Tokugawa\n",
    "        data_training['ZIP_Cosmos'] =    ZIP_Cosmos\n",
    "        data_training['ZIP_Centaur'] =    ZIP_Centaur   \n",
    "       \n",
    "    \n",
    "        Majority_change = []\n",
    "        pZIP_Ebony  = []\n",
    "        pZIP_Odyssey= []\n",
    "        pZIP_Tokugawa= []\n",
    "        pZIP_Cosmos= []\n",
    "        pZIP_Centaur= []\n",
    "        pZIP_pref1 = []\n",
    "        pZIP_pref2 = []\n",
    "        pZIP_lev_pref1 = []\n",
    "        pZIP_lev_pref2 = []\n",
    "        for i in data_training['mvar32'].values:  #    majority[i] = [l.count('Ebony'),l.count('Odyssey'),l.count('Tokugawa'),l.count('Cosmos'),l.count('Centaur')]\n",
    "            pZIP_pref1.append(zips[i][0])\n",
    "            pZIP_pref2.append(zips[i][2])\n",
    "            pZIP_lev_pref1.append(zips[i][1])\n",
    "            pZIP_lev_pref2.append(zips[i][3])                  \n",
    "            pZIP_Ebony.append(majority[i][0])\n",
    "            pZIP_Odyssey.append(majority[i][1])\n",
    "            pZIP_Tokugawa.append(majority[i][2])\n",
    "            pZIP_Cosmos.append(majority[i][3])\n",
    "            pZIP_Centaur.append(majority[i][4])  \n",
    "        data_training['pZIP_pref1'] =    pZIP_pref1\n",
    "        data_training['pZIP_pref2'] =    pZIP_pref2\n",
    "        data_training['pZIP_lev_pref1'] =    pZIP_lev_pref1\n",
    "        data_training['pZIP_lev_pref2'] =    pZIP_lev_pref2\n",
    "        data_training['pZIP_Ebony'] =    pZIP_Ebony\n",
    "        data_training['pZIP_Odyssey'] =    pZIP_Odyssey\n",
    "        data_training['pZIP_Tokugawa'] =    pZIP_Tokugawa\n",
    "        data_training['pZIP_Cosmos'] =    pZIP_Cosmos\n",
    "        data_training['pZIP_Centaur'] =   pZIP_Centaur       \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for data_training in [df1,df2,df3]: \n",
    "    Majority_change = []\n",
    "    for i in range(len(data_training)):\n",
    "        if data_training['pZIP_pref1'].values[i] == data_training['ZIP_pref1'].values[i]:\n",
    "            Majority_change.append(0)\n",
    "        else:\n",
    "            Majority_change.append(1)\n",
    "    data_training['Majority_change'] = Majority_change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "data_training = df1\n",
    "data_testing = df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1.to_csv('T2.csv')\n",
    "df2.to_csv('V2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_training = pandas.read_csv('T2.csv')\n",
    "data_testing = pandas.read_csv('V2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df1 = data_training\n",
    "History_vote = []\n",
    "for i in range(len(df1)):\n",
    "    if df1['actual_vote'].values[i] == df1['actual_vote'].values[i]: \n",
    "        if df1['actual_vote'].values[i] != df1['party_voted_past'].values[i]:\n",
    "             History_vote.append([df1['lev_pref1'].values[i],df1['lev_pref2'].values[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "L1 = data_training['lev_pref1'].values\n",
    "L2 = data_training['lev_pref2'].values\n",
    "M1 = data_testing['lev_pref1'].values\n",
    "M2 = data_testing['lev_pref2'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "L = L1 - L2\n",
    "M = M1 - M2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Train_SET1 = []\n",
    "Test_SET1 = []\n",
    "for i in range(len(L)):\n",
    "    if L1[i]>=5 and L[i]>=5:\n",
    "        Train_SET1.append(i)\n",
    "for i in range(len(M)):\n",
    "    if M1[i]>4 and M[i]>3:\n",
    "        Test_SET1.append(i)\n",
    "import numpy as np\n",
    "x = set(list(np.arange(len(L))))\n",
    "Train_SET2 = x - set(Train_SET1) \n",
    "data_training_set2 = data_training.drop(data_training.index[Train_SET1])\n",
    "data_training_set1 = data_training.drop(data_training.index[list(Train_SET2)])\n",
    "#data_training_set2 = data_training.ix[Rows]\n",
    "#data_training_set2 = data_training.ix[Rows2]\n",
    "\n",
    "x = set(list(np.arange(len(M))))\n",
    "Test_SET2 = x - set(Test_SET1) \n",
    "data_testing_set2 = data_testing.drop(data_testing.index[Test_SET1])\n",
    "data_testing_set1 = data_testing.drop(data_testing.index[list(Test_SET2)])\n",
    "#data_testing_set1 = data_testing[data_testing.Outlayers != 1] \n",
    "#data_testing_set2 = data_testing[data_testing.Outlayers != -1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(data_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(data_training_set2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(History_vote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1 = data_training\n",
    "History_vote = []\n",
    "for i in range(len(df1)):\n",
    "    if df1['party_voted_past'].values[i] == df1['Cit_pref1'].values[i]: \n",
    "        if df1['actual_vote'].values[i] != df1['party_voted_past'].values[i] and df1['actual_vote'].values[i] != df1['Cit_pref1'].values[i]:\n",
    "             History_vote.append(1)\n",
    "        else:\n",
    "             History_vote.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9540"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(History_vote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18761061946902655"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(History_vote)/float(len(History_vote))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df1 = data_training_set2\n",
    "History_vote = []\n",
    "Rows = []\n",
    "for i in range(len(df1)):\n",
    "    if df1['actual_vote'].values[i] != df1['Cit_pref1'].values[i] and df1['actual_vote'].values[i] != df1['party_voted_past'].values[i] and df1['actual_vote'].values[i] != df1['ZIP_pref1'].values[i] and df1['actual_vote'].values[i] != df1['ZIP_pref2'].values[i] and df1['actual_vote'].values[i] != df1['Cit_pref2'].values[i]:\n",
    "        History_vote.append(1)\n",
    "        Rows.append(i)\n",
    "    else:\n",
    "        History_vote.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_features_test= ['Ebony','Odyssey','Tokugawa','mvar32','mvar33','Cosmos','Centaur','city_change','ZIP_pref1','ZIP_pref2','mvar34','mvar35','mvar29','lev_pref1','lev_pref2','ZIP_lev_pref1','ZIP_lev_pref2','mvar27','mvar28','mvar31']\n",
    "Feature = data_testing_set1.columns[3:28]\n",
    "new_features_train= ['Ebony','Odyssey','Tokugawa','Cosmos','Centaur','mvar32','mvar33','city_change','ZIP_pref1','ZIP_pref2','mvar34','mvar35','mvar29','lev_pref1','lev_pref2','ZIP_lev_pref1','ZIP_lev_pref2','mvar27','mvar28','mvar31']\n",
    "new_features_test = new_features_test +  list(Feature)\n",
    "new_features_train = new_features_train + list(Feature)\n",
    "Y_SET1 = data_training_set1['actual_vote'].values\n",
    "Y_SET2 = data_training_set2['actual_vote'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "MinMax = MinMaxScaler()\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "for df in [data_testing_set1,data_testing_set2,data_training_set1,data_training_set2]:\n",
    "            for i in Feature:\n",
    "                df[i] = df[i].astype(str)\n",
    "            df = df[new_features_train][:]\n",
    "            df['mvar27'] = df['mvar27'].apply(str)\n",
    "            #df['lev_pref1'] = MinMax.fit_transform(df['lev_pref1'].values)\n",
    "            #df['ZIP_lev_pref1'] = MinMax.fit_transform(df['ZIP_lev_pref1'].values)\n",
    "            #df['ZIP_lev_pref2'] = MinMax.fit_transform(df['ZIP_lev_pref2'].values)\n",
    "            #df['lev_pref2'] = MinMax.fit_transform(df['lev_pref2'].values)\n",
    "            df['mvar35'] = MinMax.fit_transform(df['mvar35'].values)\n",
    "            df['mvar34'] = MinMax.fit_transform(df['mvar34'].values)\n",
    "            df['mvar28'] = MinMax.fit_transform(df['mvar28'].values)\n",
    "            df['mvar31'] = MinMax.fit_transform(df['mvar31'].values)\n",
    "            df['mvar29'] = MinMax.fit_transform(df['mvar29'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ENCODING\n",
    "data_training_1 = data_training_set1[new_features_test][:]\n",
    "data_testing_1 = data_testing_set1[new_features_test][:]\n",
    "X_train = data_training_1.to_dict('records')\n",
    "X_test = data_testing_1.to_dict('records')\n",
    "X_tr = []\n",
    "X_te = []\n",
    "X_tr.extend(X_train)\n",
    "X_te.extend(X_test)\n",
    "X_total = X_tr + X_te\n",
    "#One Hot Encoding \n",
    "from sklearn.feature_extraction import DictVectorizer \n",
    "enc = DictVectorizer(sparse = True)\n",
    "X_encoded_total =enc.fit_transform(X_total)\n",
    "X_encoded_train_set1 =X_encoded_total[:len(X_tr)]\n",
    "X_encoded_test_set1 =X_encoded_total[len(X_tr):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#ENCODING\n",
    "data_training_2 = data_training_set2[new_features_test][:]\n",
    "data_testing_2 = data_testing_set2[new_features_test][:]\n",
    "X_train = data_training_2.to_dict('records')\n",
    "X_test = data_testing_2.to_dict('records')\n",
    "X_tr = []\n",
    "X_te = []\n",
    "X_tr.extend(X_train)\n",
    "X_te.extend(X_test)\n",
    "X_total = X_tr + X_te\n",
    "#One Hot Encoding \n",
    "from sklearn.feature_extraction import DictVectorizer \n",
    "enc = DictVectorizer(sparse = True)\n",
    "X_encoded_total =enc.fit_transform(X_total)\n",
    "X_encoded_train_set2 =X_encoded_total[:len(X_tr)]\n",
    "X_encoded_test_set2 =X_encoded_total[len(X_tr):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression,RandomizedLogisticRegression\n",
    "clf_SET1 =LogisticRegression(solver = 'newton-cg',fit_intercept = False)\n",
    "clf_SET2 =LogisticRegression(solver = 'newton-cg',fit_intercept = False)\n",
    "clf_SET1.fit(X_encoded_train_set1,Y_SET1)\n",
    "clf_SET2.fit(X_encoded_train_set2,Y_SET2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sol_SET1 =clf_SET1.predict(X_encoded_test_set1.toarray())\n",
    "sol_SET2 =clf_SET2.predict(X_encoded_test_set2.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_testing_set2['Predicted'] = sol_SET2\n",
    "data_testing_set1['Predicted'] = sol_SET1\n",
    "DF1 = data_testing_set1[['citizen_id','Predicted']]\n",
    "DF2 = data_testing_set2[['citizen_id','Predicted']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "frames = [DF1, DF2]\n",
    "result = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result.to_csv('Fi8.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas \n",
    "df1 = pandas.read_csv('T.csv')\n",
    "History_vote = []\n",
    "for i in range(len(df1)):\n",
    "    if df1['actual_vote'].values[i] == df1['actual_vote'].values[i]: \n",
    "        if df1['actual_vote'].values[i] == df1['party_voted_past'].values[i]:\n",
    "             History_vote.append(1)\n",
    "        else:\n",
    "             History_vote.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = []\n",
    "for i in range(len(df1)):\n",
    "    X.append([df1['lev_pref1'].values[i],df1['lev_pref2'].values[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y = History_vote\n",
    "A = int(0.15*len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_te = X[:A]\n",
    "X_tr = X[A:]\n",
    "Y_te = History_vote[:A]\n",
    "Y_tr = History_vote[A:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier,RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf = GaussianNB()#GradientBoostingClassifier(max_depth=2,n_estimators=100,)\n",
    "clf.fit(X_te,Y_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_pr = clf.predict(History_vote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24815104676188615"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import zero_one_loss,confusion_matrix,classification_report\n",
    "zero_one_loss(Y_pr,Y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'             precision    recall  f1-score   support\\n\\n          0       0.07      0.52      0.12      1713\\n          1       0.98      0.76      0.86     49397\\n\\navg / total       0.95      0.75      0.83     51110\\n'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_report(Y_pr,Y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "        precision    recall  f1-score   support\n",
    "0       0.15         0.53      0.24      3728\n",
    "1       0.95         0.77      0.85     47382\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15013"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Y_pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13972"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(Y_pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
