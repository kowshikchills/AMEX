{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "data_training = pandas.read_csv('T2.csv') \n",
    "data_testing = pandas.read_csv('V2.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas \n",
    "df1 = data_training\n",
    "SPLIT1 = []\n",
    "SPLIT2 = []\n",
    "for i in range(len(df1)):\n",
    "    if df1['party_voted_past'].values[i] == df1['Cit_pref1'].values[i]: \n",
    "        SPLIT1.append(i) \n",
    "    else:\n",
    "        SPLIT2.append(i) \n",
    "data_training_split1 =data_training.ix[SPLIT1]\n",
    "data_training_split2 =data_training.ix[SPLIT2]        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas \n",
    "df1 = data_testing\n",
    "SPLIT1 = []\n",
    "SPLIT2 = []\n",
    "for i in range(len(df1)):\n",
    "    if df1['party_voted_past'].values[i] == df1['Cit_pref1'].values[i]: \n",
    "        SPLIT1.append(i) \n",
    "    else:\n",
    "        SPLIT2.append(i) \n",
    "data_Validating_split1 =data_testing.ix[SPLIT1]\n",
    "data_validating_split2 =data_testing.ix[SPLIT2] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_training_split2.to_csv('trainsplit2.csv')\n",
    "data_validating_split2.to_csv('validsplit21.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 894,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "L1 = data_testing_split1['lev_pref1'].values\n",
    "L2 = data_testing_split1['lev_pref2'].values\n",
    "M1 = data_training_split1['lev_pref1'].values\n",
    "M2 = data_training_split1['lev_pref2'].values\n",
    "L = L1 - L2\n",
    "M = M1 - M2\n",
    "Train_SET1 = []\n",
    "Test_SET1 = []\n",
    "for i in range(len(L)):\n",
    "    if L1[i]>=4 and L[i]>=3:\n",
    "        Train_SET1.append(i)\n",
    "for i in range(len(M)):\n",
    "    if M1[i]>4 and M[i]>3:\n",
    "        Test_SET1.append(i)\n",
    "import numpy as np\n",
    "x = set(list(np.arange(len(L))))\n",
    "Train_SET2 = x - set(Train_SET1) \n",
    "data_Validating_set12 = data_Validating_split1.drop(data_testing_split1.index[Train_SET1])\n",
    "ddata_Validating_set11 = data_Validating_split1.drop(data_testing_split1.index[list(Train_SET2)])\n",
    "\n",
    "x = set(list(np.arange(len(M))))\n",
    "Test_SET2 = x - set(Test_SET1) \n",
    "data_training_set12 = data_training_split1.drop(data_training_split1.index[Test_SET1])\n",
    "data_training_set11 = data_training_split1.drop(data_training_split1.index[list(Test_SET2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Feature = ['party_voted_past','Cit_pref1','lev_pref1','lev_pref2','ZIP_pref1','ZIP_pref2','ZIP_lev_pref1','ZIP_lev_pref2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_features= ['Ebony','Odyssey','Tokugawa','Cosmos','Centaur','Cit_pref2','city_change','ZIP_pref1','ZIP_pref2','mvar34','mvar35','mvar29','lev_pref1','lev_pref2','ZIP_lev_pref1','ZIP_lev_pref2','mvar27','mvar28','mvar31']\n",
    "feature = data_validating_split2.columns[3:29]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Feature = new_features +  list(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_train = data_training[:]\n",
    "df_test =  data_validating_split2[:]\n",
    "Y = df_train['actual_vote'].values\n",
    "del df_train['actual_vote']\n",
    "del df_train['citizen_id']\n",
    "del df_test['citizen_id']\n",
    "for i in feature:\n",
    "    df_train[i] = df_train[i].astype(str)\n",
    "    df_test[i] = df_test[i].astype(str) \n",
    "\n",
    "#ENCODING\n",
    "df_train = df_train[Feature]\n",
    "df_test = df_test[Feature]\n",
    "X_train = df_train.to_dict('records')\n",
    "X_test = df_test.to_dict('records')\n",
    "X_tr = []\n",
    "X_te = []\n",
    "X_tr.extend(X_train)\n",
    "X_te.extend(X_test)\n",
    "X_total = X_tr + X_te\n",
    "\n",
    "\n",
    "#One Hot Encoding \n",
    "from sklearn.feature_extraction import DictVectorizer \n",
    "enc = DictVectorizer(sparse = True)\n",
    "X_encoded_total =enc.fit_transform(X_total)\n",
    "X_encoded_train =X_encoded_total[:len(X_tr)]\n",
    "X_encoded_test =X_encoded_total[len(X_tr):]\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf11 = GradientBoostingClassifier(n_estimators=500,learning_rate=0.01)\n",
    "clf10 = LogisticRegression(solver = 'newton-cg',fit_intercept = False)\n",
    "clf10.fit(X_encoded_train,Y)\n",
    "\n",
    "sol = clf10.predict(X_encoded_test.toarray())\n",
    "df_test['predicted'] = sol\n",
    "data_validating_split2['predicted'] = sol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_Validating_split1['predicted'] = data_Validating_split1['party_voted_past']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3318"
      ]
     },
     "execution_count": 1120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_validating_split2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 13] Permission denied: 'Fi11.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1121-773973a42758>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'citizen_id'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Fi11.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\Kowshik\\AppData\\Local\\Enthought\\Canopy\\User\\lib\\site-packages\\pandas\\core\\frame.pyc\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, tupleize_cols, date_format, doublequote, escapechar, decimal, **kwds)\u001b[0m\n\u001b[0;32m   1330\u001b[0m                                      \u001b[0mescapechar\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mescapechar\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1331\u001b[0m                                      decimal=decimal)\n\u001b[1;32m-> 1332\u001b[1;33m         \u001b[0mformatter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1333\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1334\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpath_or_buf\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Kowshik\\AppData\\Local\\Enthought\\Canopy\\User\\lib\\site-packages\\pandas\\core\\format.pyc\u001b[0m in \u001b[0;36msave\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1479\u001b[0m             f = _get_handle(self.path_or_buf, self.mode,\n\u001b[0;32m   1480\u001b[0m                                 \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1481\u001b[1;33m                                 compression=self.compression)\n\u001b[0m\u001b[0;32m   1482\u001b[0m             \u001b[0mclose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Kowshik\\AppData\\Local\\Enthought\\Canopy\\User\\lib\\site-packages\\pandas\\io\\common.pyc\u001b[0m in \u001b[0;36m_get_handle\u001b[1;34m(path, mode, encoding, compression)\u001b[0m\n\u001b[0;32m    354\u001b[0m                 \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'replace'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    355\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 356\u001b[1;33m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    357\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    358\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIOError\u001b[0m: [Errno 13] Permission denied: 'Fi11.csv'"
     ]
    }
   ],
   "source": [
    "DF1 = data_Validating_split1[['citizen_id','predicted']]\n",
    "DF2 = data_validating_split2[['citizen_id','predicted']]\n",
    "import pandas as pd\n",
    "frames = [DF1, DF2]\n",
    "result = pd.concat(frames)\n",
    "result.sort(columns='citizen_id')\n",
    "result.to_csv('Fi11.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DF2 = data_validating_split2[['citizen_id','predicted']]\n",
    "DF2.to_csv('Fioooh.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1 = pandas.read_csv('Fioooh.csv')\n",
    "df2 = pandas.read_csv('lead23_train5equal_3.75.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df2.update(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df2.to_csv('finaltry.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1073,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1 = data_Validating_split1\n",
    "History_vote = []\n",
    "histogramvoted = []\n",
    "histogramnotvoted = []\n",
    "for i in range(len(df1)): \n",
    "        if df1['predicted'].values[i] == df1['party_voted_past'].values[i] :#or df1['actual_vote'].values[i] == df1['party_voted_past'].values[i]:\n",
    "                History_vote.append(1)                      \n",
    "        else:\n",
    "                History_vote.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1092,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17889"
      ]
     },
     "execution_count": 1092,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(History_vote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1093,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3196"
      ]
     },
     "execution_count": 1093,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(History_vote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 868,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1 = data_training_split2\n",
    "History_vote = []\n",
    "histogramvoted = []\n",
    "histogramnotvoted = []\n",
    "for i in range(len(df1)): \n",
    "        if df1['actual_vote'].values[i] == df1['pZIP_pref1'].values[i]:\n",
    "                History_vote.append(1)                      \n",
    "        else:\n",
    "                History_vote.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 869,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3415"
      ]
     },
     "execution_count": 869,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(History_vote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "data = histogramnotvoted\n",
    "\n",
    "# fixed bin size\n",
    "bins = np.arange(-20, 20, 1) # fixed bin size\n",
    "\n",
    "plt.xlim([min(data)-5, max(data)+5])\n",
    "\n",
    "plt.hist(data, bins=bins, alpha=0.5)\n",
    "plt.title('Random Gaussian data (fixed bin size)')\n",
    "plt.xlabel('variable X (bin size = 5)')\n",
    "plt.ylabel('count')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "L1 = data_testing_split1['lev_pref1'].values\n",
    "L2 = data_testing_split1['lev_pref2'].values\n",
    "M1 = data_training_split1['lev_pref1'].values\n",
    "M2 = data_training_split1['lev_pref2'].values\n",
    "L = L1 - L2\n",
    "M = M1 - M2\n",
    "Train_SET1 = []\n",
    "Test_SET1 = []\n",
    "for i in range(len(L)):\n",
    "    if L1[i]>=4 and L[i]>=4:\n",
    "        Train_SET1.append(i)\n",
    "for i in range(len(M)):\n",
    "    if M1[i]>4 and M[i]>4:\n",
    "        Test_SET1.append(i)\n",
    "import numpy as np\n",
    "x = set(list(np.arange(len(L))))\n",
    "Train_SET2 = x - set(Train_SET1) \n",
    "data_testing_set12 = data_testing_split1.drop(data_testing_split1.index[Train_SET1])\n",
    "data_testing_set11 = data_testing_split1.drop(data_testing_split1.index[list(Train_SET2)])\n",
    "\n",
    "x = set(list(np.arange(len(M))))\n",
    "Test_SET2 = x - set(Test_SET1) \n",
    "data_training_set12 = data_training_split1.drop(data_training_split1.index[Test_SET1])\n",
    "data_training_set11 = data_training_split1.drop(data_training_split1.index[list(Test_SET2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "L1 = data_testing_split2['lev_pref1'].values\n",
    "L2 = data_testing_split2['lev_pref2'].values\n",
    "M1 = data_testing_split2['lev_pref1'].values\n",
    "M2 = data_testing_split2['lev_pref2'].values\n",
    "L = L1 - L2\n",
    "M = M1 - M2\n",
    "Train_SET1 = []\n",
    "Test_SET1 = []\n",
    "for i in range(len(L)):\n",
    "    if L1[i]>=4 and L[i]>=4:\n",
    "        Train_SET1.append(i)\n",
    "for i in range(len(M)):\n",
    "    if M1[i]>4 and M[i]>4:\n",
    "        Test_SET1.append(i)\n",
    "import numpy as np\n",
    "x = set(list(np.arange(len(L))))\n",
    "Train_SET2 = x - set(Train_SET1) \n",
    "data_testing_set22 = data_testing_split2.drop(data_testing_split2.index[Train_SET1])\n",
    "data_testing_set21 = data_testing_split2.drop(data_testing_split2.index[list(Train_SET2)])\n",
    "\n",
    "x = set(list(np.arange(len(M))))\n",
    "Test_SET2 = x - set(Test_SET1) \n",
    "data_training_set22 = data_training_split2.drop(data_training_split2.index[Test_SET1])\n",
    "data_training_set21 = data_training_split2.drop(data_training_split2.index[list(Test_SET2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_testing_set11.to_csv('test11.csv')\n",
    "data_testing_set12.to_csv('test12.csv')\n",
    "data_testing_set21.to_csv('test21.csv')\n",
    "data_testing_set22.to_csv('test22.csv')\n",
    "\n",
    "data_training_set11.to_csv('train11.csv')\n",
    "data_training_set12.to_csv('train12.csv')\n",
    "data_training_set21.to_csv('train21.csv')\n",
    "data_training_set22.to_csv('train22.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_testing_set1 = pd.concat([data_testing_set11+ data_testing_set21])\n",
    "data_testing_set2  = pd.concat([data_testing_set12 + data_testing_set22]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_training_set1 = pd.concat([data_training_set11+data_training_set21])\n",
    "data_training_set2  = pd.concat([data_training_set12 + data_training_set22]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_testing_set1.to_csv('X1.csv')\n",
    "data_testing_set2.to_csv('X2.csv')\n",
    "data_training_set1.to_csv('Y1.csv')\n",
    "data_training_set2.to_csv('Y2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1           Ebony\n",
       "3         Centaur\n",
       "6          Cosmos\n",
       "19        Centaur\n",
       "27         Cosmos\n",
       "33         Cosmos\n",
       "39        Odyssey\n",
       "44         Cosmos\n",
       "48         Cosmos\n",
       "62          Ebony\n",
       "130       Odyssey\n",
       "132         Ebony\n",
       "146       Odyssey\n",
       "202        Cosmos\n",
       "204        Cosmos\n",
       "226       Odyssey\n",
       "232       Odyssey\n",
       "235         Ebony\n",
       "259       Centaur\n",
       "272        Cosmos\n",
       "278         Ebony\n",
       "293       Odyssey\n",
       "305      Tokugawa\n",
       "334       Centaur\n",
       "369       Centaur\n",
       "381       Centaur\n",
       "382       Centaur\n",
       "383         Ebony\n",
       "385        Cosmos\n",
       "404       Odyssey\n",
       "           ...   \n",
       "20946      Cosmos\n",
       "20959       Ebony\n",
       "20968       Ebony\n",
       "20977     Odyssey\n",
       "21001       Ebony\n",
       "21002    Tokugawa\n",
       "21013      Cosmos\n",
       "21028     Odyssey\n",
       "21030     Odyssey\n",
       "21037      Cosmos\n",
       "21038      Cosmos\n",
       "21049    Tokugawa\n",
       "21052      Cosmos\n",
       "21060     Odyssey\n",
       "21068      Cosmos\n",
       "21076      Cosmos\n",
       "21114       Ebony\n",
       "21119     Odyssey\n",
       "21128      Cosmos\n",
       "21131      Cosmos\n",
       "21145       Ebony\n",
       "21147     Odyssey\n",
       "21156     Odyssey\n",
       "21159    Tokugawa\n",
       "21161     Odyssey\n",
       "21163      Cosmos\n",
       "21183    Tokugawa\n",
       "21199    Tokugawa\n",
       "21203     Odyssey\n",
       "21204     Odyssey\n",
       "Name: actual_vote, dtype: object"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_testing_set22['actual_vote']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
